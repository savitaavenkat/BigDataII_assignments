{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9 - NLP using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get to work with recurrent network architectures with application to language processing tasks and observe behaviour of the learning using tensorboard visualization.\n",
    "\n",
    "You'll learn to use\n",
    "\n",
    " * word embeddings,\n",
    " * LSTMs,\n",
    " * tensorboard visualization to develop and tune deep learning architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the deep learning environment in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same kind of preparation as in [Assignment 6](../A6/A6.html) we are going to use [pytorch](http://pytorch.org) for the deep learning aspects of the assignment. \n",
    "\n",
    "There is a `pytorch` setup in the big data lab under the globally available anaconda installation.\n",
    "However, it is recommended that you use the custom **py36** conda environment that contains all python package dependencies that are relevant for this assignment (and also nltk, gensim, tensorflow, keras, and tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either you load it directly\n",
    "```\n",
    "source activate /usr/shared/CMPT/big-data/condaenv/py36\n",
    "```\n",
    "or you prepare\n",
    "```\n",
    "cd ~\n",
    "mkdir -p .conda/envs\n",
    "ln -s /usr/shared/CMPT/big-data/condaenv/py36 .conda/envs\n",
    "```\n",
    "and from thereon simply use\n",
    "```\n",
    "source activate py36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there are some relevant datasets available in our shared folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "bdenv_loc = '/usr/shared/CMPT/big-data'\n",
    "bdata = os.path.join(bdenv_loc,'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Explore Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are mappings between words and multi-dimensional vectors, where the difference between two word vectors has some relationship with the meaning of the corresponding words, i.e. words that are similar in meaning are mapped closely together (ideally). This part of the assignment should enable you to\n",
    "\n",
    "* Load a pretrained word embedding\n",
    "* Perform basic operations, such as distance queries and evaluate simple analogies\n",
    "\n",
    "Note, each of the tasks below can be addressed with one or two lines of code using the [word2vec API in gensim](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google's pre-trained Word2Vec model has been loaded..\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# Load Google's pre-trained Word2Vec model, trained on news articles\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    os.path.join(bdata,'GoogleNews-vectors-negative300.bin'), binary=True)\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(\"Google's pre-trained Word2Vec model has been loaded..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a vector representation for a word of your choice.\n",
    "To confirm that this worked, print out the number of elements of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities or length of the vector representation of the word \"Beautiful\" : 300\n",
      "\n",
      "Vector representation of \"Beautiful\" : [-0.01831055  0.05566406 -0.01153564  0.07275391  0.15136719 -0.06176758\n",
      "  0.20605469 -0.15332031 -0.05908203  0.22851562 -0.06445312 -0.22851562\n",
      " -0.09472656 -0.03344727  0.24707031  0.05541992 -0.00921631  0.1328125\n",
      " -0.15429688  0.08105469 -0.07373047  0.24316406  0.12353516 -0.09277344\n",
      "  0.08203125  0.06494141  0.15722656  0.11279297 -0.0612793  -0.296875\n",
      " -0.13378906  0.234375    0.09765625  0.17773438  0.06689453 -0.27539062\n",
      "  0.06445312 -0.13867188 -0.08886719  0.171875    0.07861328 -0.10058594\n",
      "  0.23925781  0.03808594  0.18652344 -0.11279297  0.22558594  0.10986328\n",
      " -0.11865234  0.02026367  0.11376953  0.09570312  0.29492188  0.08251953\n",
      " -0.05444336 -0.0090332  -0.0625     -0.17578125 -0.08154297  0.01062012\n",
      " -0.04736328 -0.08544922 -0.19042969 -0.30273438  0.07617188  0.125\n",
      " -0.05932617  0.03833008 -0.03564453  0.2421875   0.36132812  0.04760742\n",
      "  0.00631714 -0.03088379 -0.13964844  0.22558594 -0.06298828 -0.02636719\n",
      "  0.1171875   0.33398438 -0.07666016 -0.06689453  0.04150391 -0.15136719\n",
      " -0.22460938  0.03320312 -0.15332031  0.07128906  0.16992188  0.11572266\n",
      " -0.13085938  0.12451172 -0.20410156  0.04736328 -0.296875   -0.17480469\n",
      "  0.00872803 -0.04638672  0.10791016 -0.203125   -0.27539062  0.2734375\n",
      "  0.02563477 -0.11035156  0.0625      0.1953125   0.16015625 -0.13769531\n",
      " -0.09863281 -0.1953125  -0.22851562  0.25390625  0.00915527 -0.03857422\n",
      "  0.3984375  -0.1796875   0.03833008 -0.24804688  0.03515625  0.03881836\n",
      "  0.03442383 -0.04101562  0.20214844 -0.03015137 -0.09619141  0.11669922\n",
      " -0.06738281  0.0625      0.10742188  0.25585938 -0.21777344  0.05639648\n",
      " -0.0065918   0.16113281  0.11865234 -0.03088379 -0.11572266  0.02685547\n",
      "  0.03100586  0.09863281  0.05883789  0.00634766  0.11914062  0.07324219\n",
      " -0.01586914  0.18457031  0.05322266  0.19824219 -0.22363281 -0.25195312\n",
      "  0.15039062  0.22753906  0.05737305  0.16992188 -0.22558594  0.06494141\n",
      "  0.11914062 -0.06640625 -0.10449219 -0.07226562 -0.16992188  0.0625\n",
      "  0.14648438  0.27148438 -0.02172852 -0.12695312  0.18457031 -0.27539062\n",
      " -0.36523438 -0.03491211 -0.18554688  0.23828125 -0.13867188  0.00296021\n",
      "  0.04272461  0.13867188  0.12207031  0.05957031 -0.22167969 -0.18945312\n",
      " -0.23242188 -0.28710938 -0.00866699 -0.16113281 -0.24316406  0.05712891\n",
      " -0.06982422  0.00053406 -0.10302734 -0.13378906 -0.16113281  0.11621094\n",
      "  0.31640625 -0.02697754 -0.01574707  0.11425781 -0.04174805  0.05908203\n",
      "  0.02661133 -0.08642578  0.140625    0.09228516 -0.25195312 -0.31445312\n",
      " -0.05688477  0.01031494  0.0234375  -0.02331543 -0.08056641  0.01269531\n",
      " -0.34179688  0.17285156 -0.16015625  0.07763672 -0.03088379  0.11962891\n",
      "  0.11767578  0.20117188 -0.01940918  0.02172852  0.23046875  0.28125\n",
      " -0.17675781  0.02978516  0.08740234 -0.06176758  0.00939941 -0.09277344\n",
      " -0.203125    0.13085938 -0.13671875 -0.00500488 -0.04296875  0.12988281\n",
      "  0.3515625   0.0402832  -0.12988281 -0.03173828  0.28515625  0.18261719\n",
      "  0.13867188 -0.16503906 -0.26171875 -0.04345703  0.0100708   0.08740234\n",
      "  0.00421143 -0.1328125  -0.17578125 -0.04321289 -0.015625    0.16894531\n",
      "  0.25        0.37109375  0.19921875 -0.36132812 -0.10302734 -0.20800781\n",
      " -0.20117188 -0.01519775 -0.12207031 -0.12011719 -0.07421875 -0.04345703\n",
      "  0.14160156  0.15527344 -0.03027344 -0.09326172 -0.04589844  0.16796875\n",
      " -0.03027344  0.09179688 -0.10058594  0.20703125  0.11376953 -0.12402344\n",
      "  0.04003906  0.06933594 -0.34570312  0.03881836  0.16210938  0.05761719\n",
      " -0.12792969 -0.05810547  0.03857422 -0.11328125 -0.1953125  -0.28125\n",
      " -0.13183594  0.15722656 -0.09765625  0.09619141 -0.09960938 -0.00285339\n",
      " -0.03637695  0.15429688  0.06152344 -0.34570312  0.11083984  0.03344727]\n"
     ]
    }
   ],
   "source": [
    "word = 'beautiful'\n",
    "word_vec = model.get_vector(word)\n",
    "print('Number of entities or length of the vector representation of the word \"Beautiful\" :',len(word_vec))\n",
    "print('\\nVector representation of \"Beautiful\" :', word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the 10 words that are closest in the embedding to the word vector you produced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gorgeous', 0.8353004455566406),\n",
       " ('lovely', 0.8106936812400818),\n",
       " ('stunningly_beautiful', 0.7329413890838623),\n",
       " ('breathtakingly_beautiful', 0.7231341600418091),\n",
       " ('wonderful', 0.6854087114334106),\n",
       " ('fabulous', 0.6700063943862915),\n",
       " ('loveliest', 0.6612576246261597),\n",
       " ('prettiest', 0.6595001816749573),\n",
       " ('beatiful', 0.6593325138092041),\n",
       " ('magnificent', 0.6591403484344482)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['beautiful'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the nearest neighbours similar in meaning?\n",
    "Try different seed words, until you find one whose neighbourhood looks OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a combination of positive and negative words, find out which word is most\n",
    "similar to `woman + king - man`. Note that gensim's API allows you to combine positive and negative words without explicitly obtaing their vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that the results of most word analogy combinations don't work as well as we'd hope.\n",
    "\n",
    "Explore a bit and *show two more cases* where the output of gensim's built-in word vector algebra looks somewhat meaningful, i.e. show more word analogy examples or produce lists of words where a word that doesn't match is identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baby', 0.6736369132995605)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['child', 'puppy'], negative=['dog'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puppy', 0.7065186500549316)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['cat', 'dog'], negative=['lion'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dahlia', 0.5530032515525818)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['orchid', 'apple'], negative=['fruit'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sequence modeling with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will get to use a learning and a rule-based model of text sentiment analysis. To keep things simple, you will receive almost all the code and are just left with the task to tune the given algorithms, see the part about instrumentation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a simple LSTM model that is capable of producing a label for a sequence of vector encoded words, based on code from [this repo](https://github.com/clairett/pytorch-sentiment-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size,\n",
    "                 use_gpu, batch_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"Prepare individual layers\"\"\"\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=bidirectional)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*self.num_directions, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"Choose appropriate size and type of hidden layer\"\"\"\n",
    "        # first is the hidden h\n",
    "        # second is the cell c\n",
    "        if self.use_gpu:\n",
    "            return (Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim)),\n",
    "                    Variable(torch.zeros(self.num_directions, self.batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \"\"\"Use the layers of this model to propagate input and return class log probabilities\"\"\"\n",
    "        if self.use_gpu:\n",
    "            sentence = sentence.cuda()\n",
    "        x = self.embeddings(sentence).view(len(sentence), self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        \n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y, dim=0)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time, random\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm.write = print\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('latin-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.frombuffer(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs\n",
    "\n",
    "\n",
    "def get_accuracy(truth, pred):\n",
    "    assert len(truth) == len(pred)\n",
    "    right = 0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i].item() == pred[i]:\n",
    "            right += 1.0\n",
    "    return right / len(truth)\n",
    "\n",
    "\n",
    "def train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in tqdm(train_iter, desc='Train epoch '+str(epoch+1)):\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        if USE_GPU:\n",
    "            pred_label = pred.data.max(1)[1].cpu().numpy()\n",
    "        else:\n",
    "            pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data.item()\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train_epoch(model, train_iter, loss_function, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    count = 0\n",
    "    for batch in train_iter:\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        if USE_GPU:\n",
    "            pred_label = pred.data.max(1)[1].cpu().numpy()\n",
    "        else:\n",
    "            pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data.item()\n",
    "        count += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_iter)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, data, loss_function, name):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    for batch in data:\n",
    "        sent, label = batch.text, batch.label\n",
    "        label.data.sub_(1)\n",
    "        truth_res += list(label.data)\n",
    "        model.batch_size = len(label.data)\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred = model(sent)\n",
    "        if USE_GPU:\n",
    "            pred_label = pred.data.max(1)[1].cpu().numpy()\n",
    "        else:\n",
    "            pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res += [x for x in pred_label]\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data.item()\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ': loss %.2f acc %.1f' % (avg_loss, acc*100))\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def load_sst(text_field, label_field, batch_size, use_gpu=True):\n",
    "    train, dev, test = data.TabularDataset.splits(path=os.path.join(bdata,'sst2'), train='train.tsv',\n",
    "                                                  validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "                                                  fields=[('text', text_field), ('label', label_field)])\n",
    "\n",
    "#     train, dev, test = data.TabularDataset.splits(path=os.path.join('./'), train='train.tsv',\n",
    "#                                                   validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "#                                                   fields=[('text', text_field), ('label', label_field)])\n",
    "    text_field.build_vocab(train, dev, test)\n",
    "    label_field.build_vocab(train, dev, test)\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test),\n",
    "                                                                 batch_sizes=(batch_size, len(dev), len(test)),\n",
    "                                                                 sort_key=lambda x: len(x.text), repeat=False,\n",
    "                                                                 device=torch.device(\"cuda\" if use_gpu else \"cpu\"))\n",
    "    return train_iter, dev_iter, test_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** After instrumentation with the summary writer (see further below), tune these parameters to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "# EMBEDDING_TYPE = 'glove'\n",
    "EMBEDDING_TYPE = 'word2vec'\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "USE_BILSTM = True\n",
    "DROPOUT = .20\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "train_iter, dev_iter, test_iter = load_sst(text_field, label_field, BATCH_SIZE, USE_GPU)\n",
    "\n",
    "model = LSTMSentiment(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                      vocab_size=len(text_field.vocab), label_size=len(label_field.vocab)-1,\\\n",
    "                      use_gpu=USE_GPU, batch_size=BATCH_SIZE, dropout=DROPOUT, bidirectional=USE_BILSTM)\n",
    "\n",
    "if USE_GPU:\n",
    "    model = model.cuda()\n",
    "\n",
    "best_model = model\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below lets you try other embedding types, but for this assignment it is fine to keep using word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'glove' in EMBEDDING_TYPE:\n",
    "    #text_field.vocab.load_vectors('glove.6B.{}d'.format(EMBEDDING_DIM))\n",
    "    text_field.vocab.load_vectors('glove.twitter.27B.100d')\n",
    "    if USE_GPU:\n",
    "        model.embeddings.weight.data = text_field.vocab.vectors.cuda()\n",
    "    else:\n",
    "        model.embeddings.weight.data = text_field.vocab.vectors\n",
    "    #model.embeddings.embed.weight.requires_grad = False\n",
    "elif 'word2vec' in EMBEDDING_TYPE:\n",
    "    word_to_idx = text_field.vocab.stoi\n",
    "    pretrained_embeddings = np.random.uniform(-0.25, 0.25, (len(text_field.vocab), 300))\n",
    "    pretrained_embeddings[0] = 0\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        print('Load word embeddings...')\n",
    "        word2vec = load_bin_vec(os.path.join(bdata,'GoogleNews-vectors-negative300.bin'), word_to_idx)\n",
    "#         word2vec = load_bin_vec('GoogleNews-vectors-negative300.bin', word_to_idx)\n",
    "    for word, vector in word2vec.items():\n",
    "        pretrained_embeddings[word_to_idx[word]-1] = vector\n",
    "    # text_field.vocab.load_vectors(wv_type='', wv_dim=300)\n",
    "\n",
    "    model.embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings));\n",
    "else:\n",
    "    print('Unknown embedding type {}'.format(EMBEDDING_TYPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual task (B1): Tensorboard instrumentation\n",
    "\n",
    "To get you to work with the some of the basic tools that enable development and tuning of deep learning architectures, we would like you to use Tensorboard.\n",
    "\n",
    "1. read up on how to instrument your code for profiling and visualization in [tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard), e.g. [at this blog](http://www.erogol.com/use-tensorboard-pytorch/)\n",
    "1. [partly done] use the tensorboard `SummaryWriter` to keep track of training loss for each epoch, writing to a local `runs` folder (which is the default)\n",
    "1. launch tensorboard and inspect the log folder, i.e. run `tensorboard --logdir runs` from the assignment folder\n",
    "\n",
    "Note that only point 2 requires you to write code, about 4 lines of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/savitaa/Desktop/A9/runs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\"))\n",
    "writer = SummaryWriter(comment='-{}lstm-em{}{}-hid{}-do{}-bs{}-lr{}'\n",
    "                                .format('BI' if USE_BILSTM else '',\n",
    "                                        EMBEDDING_TYPE, EMBEDDING_DIM,\n",
    "                                        HIDDEN_DIM,\n",
    "                                        DROPOUT, BATCH_SIZE, LEARNING_RATE))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaab1c0995fe42d48472da8231a10bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 1', max=692), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 2.30 acc 50.1\n",
      "Dev: loss 6.77 acc 49.4\n",
      "Test: loss 7.51 acc 50.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05122fbc584340e19c866e994a38b1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 2', max=692), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 2.27 acc 58.6\n",
      "Dev: loss 6.62 acc 73.5\n",
      "Test: loss 7.35 acc 75.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2840ff2ede904d8cbaec537e6de15c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 3', max=692), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 2.02 acc 83.6\n",
      "Dev: loss 6.57 acc 76.6\n",
      "Test: loss 7.28 acc 77.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f13d93482e4c248542db9102ad9c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 4', max=692), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 1.82 acc 93.7\n",
      "Dev: loss 6.70 acc 77.5\n",
      "Test: loss 7.39 acc 79.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dd11e6e58e49e6a3f9bfcb0d9f2e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train epoch 5', max=692), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss 1.74 acc 97.3\n",
      "Dev: loss 6.89 acc 76.1\n",
      "Final Test: loss 7.53 acc 79.8\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "trial = 0 # increment this if you manually decide to add more epochs to the current training\n",
    "for epoch in range(EPOCHS*trial,EPOCHS*(trial+1)):\n",
    "    avg_loss, acc = train_epoch_progress(model, train_iter, loss_function, optimizer, text_field, label_field, epoch)\n",
    "    tqdm.write('Train: loss %.2f acc %.1f' % (avg_loss, acc*100))\n",
    "    # TODO: add scalars for training loss and training accuracy to the summary writer\n",
    "    # call the scalars 'Train/Loss' and 'Train/Acc', respectively, and associate them with the current epoch\n",
    "    writer.add_scalar('Train Loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train Accuracy', acc, epoch)\n",
    "\n",
    "    dev_loss, dev_acc = evaluate(model, dev_iter, loss_function, 'Dev')\n",
    "    # TODO: add scalars for test loss and training accuracy to the summary writer\n",
    "    # call the scalars 'Val/Loss' and 'Val/Acc', respectively, and associate them with the current epoch\n",
    "    writer.add_scalar('Validation Loss', dev_loss, epoch)\n",
    "    writer.add_scalar('Validation Accuracy', dev_acc, epoch)\n",
    "    \n",
    "    if dev_acc > best_dev_acc:\n",
    "        if best_dev_acc > 0:\n",
    "            os.system('rm '+ out_dir + '/best_model' + '.pth')\n",
    "        best_dev_acc = dev_acc\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), out_dir + '/best_model' + '.pth')\n",
    "        # evaluate on test with the best dev performance model\n",
    "        test_acc = evaluate(best_model, test_iter, loss_function, 'Test')\n",
    "\n",
    "test_loss, test_acc = evaluate(best_model, test_iter, loss_function, 'Final Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B2: Tune the model\n",
    "\n",
    "After connecting the output of your model training and testing performance for monitoring in tensorboard. Change the model and training parameters above to improve the model performance. We would like to see variable plots of how validation accuracy evolves over a number of epochs for different parameter choices, you can stop exploring when you exceed a model accuracy of 76%.\n",
    "\n",
    "**Show a tensorboard screenshot with performance plots that combine at leat 5 different tuning attempts.** Store the screenshot as `tensorboard.png`. Then keep the best performing parameters set in this notebook for submission and evaluate the comparison with Vader below using your best model.\n",
    "\n",
    "Note, parameter and architecture tuning is an exercise that can go on for a long time. After you have tensorboard running, enabling you to observe learning progress for the algorithms in this notebook, **spend about half an hour tuning to improve the parameter choices**. Big leaps in performance actually require deeper research and may take days or months. While beyond the scope of this assignment, you now have the tools and background knowledge to do such work, if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison against Vader\n",
    "Vader is a rule-based sentiment analysis algorithm that performs quite well against more complex architectures. The test below is to see, whether LSTMs are able to beat its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/savitaa/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vader acc: 0.6880834706205381\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "da = test_iter.data()\n",
    "dat = [(d.text, d.label, ' '.join(d.text)) for d in da]\n",
    "lab_vpred = np.zeros((len(dat), 2))\n",
    "for k, (_, label, sentence) in enumerate(dat):\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    lab_vpred[k,:] = (int(ss['compound']>0), int(label))\n",
    "print('vader acc: {}'.format(1-abs(lab_vpred[:,0]-lab_vpred[:,1]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_iter.init_epoch\n",
    "batch = list(test_iter)[0]\n",
    "batch.text\n",
    "best_model.eval()\n",
    "pred = best_model(batch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Bi-)LSTM acc: 0.7929708951125756\n"
     ]
    }
   ],
   "source": [
    "labels = batch.label.data.cpu().detach() - 1\n",
    "labelsnp = labels.cpu().detach().numpy()\n",
    "prednp = pred.data.max(1)[1].cpu().numpy()\n",
    "lstm_acc = 1 - abs(prednp-labelsnp).mean()\n",
    "print('(Bi-)LSTM acc: {}'.format(lstm_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the model tuning and training in the previous task until you outperform the Vader algorithm by at least 5% in accuracy on the test set.** Note, this is not a separate task, but just additional code to check whether your tuning efforts have succeeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save [this notebook](A9.ipynb) containing all cell output and upload your submission as one `A9.ipynb` file.\n",
    "Also, include the screenshot of your tensorboard debugging session as `tensorboard.png`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
